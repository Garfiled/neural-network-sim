神经网络直观意义

线性变换
线性变换保留了直线和平行线(原点没有移动), 线性变换保留直线的同时，其他的几何性质如长度、角度、面积和体 积可能被变换改变了。从非技术意义上说，线性变换可能“拉伸”坐标系，但不会“弯曲”或“卷折”坐标系

为什么神经网络需要非线性激活函数是因为输入空间在线性变换下会保持三点直线不变的性质(或者说三点保持超平面不变性)，即空间不会发生弯曲

Google采用人工神经网络的原因： 
1. 理论上，人工神经网络可以在多维空间画出各种形状的模式分类边界，有很好的通用性。 
2. 过去20多年中，各种机器学习算法不断改进，但是人工神经网络算法很稳定几乎没变，google希望自己开发的计算工具能够设计一次长期使用。 
3. 并非所有的机器学习算法（比如贝叶斯网络）都容易并行化。


神经网络学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分、稀疏的空间去分类、回归

非线性变换的基本理论：
1、一个模式分类问题如果映射到一个高维空间将会比映射到一个低维空间更可能实现线性可分；
隐空间的维数越高，逼近就越精确。
2、增加每一层神经元的数量：增加维度，即增加线性转换能力。
3、增加神经网络层数：增加激活函数的作用次数，即增加非线性转换次数。


这个说法准确吗？
几乎所有的神经网络都可以看作为一种特殊制定的前馈神经网络，这里“特殊制定”的作用在于缩减寻找映射函数的搜索空间，也正是因为搜索空间的缩小，才使得网络可以用相对较少的数据量学习到更好的规律。


递归神经网络是在时间结构上存在共享特性的神经网络变体。
时间结构共享是递归网络的核心中的核心


一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。
单个神经元的作用：把一个n维向量空间用一个超平面分区成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。

linear regression = simple neural network

在序列信号的应用上，CNN是只响应预先设定的信号长度（输入向量的长度），RNN的响应长度是学习出来的。
CNN对特征的响应是线性的，RNN在这个递进方向上是非线性响应的。这也带来了很大的差别

RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出

RNN将在当前时刻的输入以及历史输入的基础上利用依赖关系对最终的预测结果进行预测
RNN解决前后依赖关系


given parameter, define a function
given network structure define a function set

卷积神经网络的核心思想是：局部感受野(local field)，权值共享以及时间或空间亚采样这三种思想结合起来，获得了某种程度的位移、尺度、形变不变性（？不够理解透彻？）。


卷积神经网络

Architecture wise, convnet is just a usual feed forward net, put on top of convolution layer(s). So really, convolution layer is a kind of feature extractor that can effectively learn the optimal features, which makes the linear classifier put on top of it looks good.

However, to understand the convnet better, it’s essential to get our hands dirty. So, let’s try implementing the conv layer from scratch using Numpy!


单个神经元的作用
神经网络是一种把东西进行分类的机器，它由若干个神经元组成。每个神经元就像一把剪刀，可以把两组数据一分为二；若干个神经元经过剪切、拼接、粘贴之后，最终可以把两个东西区分开（例如区分猫和狗），这就是神经网络。